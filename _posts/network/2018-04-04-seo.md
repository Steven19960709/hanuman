---
layout: post
title: 爬虫
date: 2018-04-04
tags: [node]
---

爬虫，对于想要批量搜集互联网上某一领域的信息，爬虫是一种便利的技术。

利用node来进行爬虫，需要第三方模块。

- request.js：Node的第三方模块，用于发起http请求
- cheerio：解析爬取的网页数据
- MonGoDB：用于存储爬取的数据
- Redis：维护一个消息队列，用于存放URL

## request.js

相对于Python或者java，node实现的爬虫会增加一点异步代码。

request是用来发起http请求，理论上使用原生的http模块就能实现，第三方模块更加方便。下面是request的例子。  

    const request = require('request');
    request("http://www.xxx.com", function (error, response, body) {
        console.log(err);
        consoel.log(response.statusCode);
    });

request第一个参数是一个url，也可以是一个对象代表HTTP header，例如：

    const option = {
        url: "example.com",
        headers: {
            "Accept": "text/html",
            "Accept-charset": "utf8",
            "Cache-Control": "max-age=0",
            "Connection": "keep-alive",
            "Accept-Lanuage": "zh-CN, zh; q=0.8, en;",
            "User-Agent": "expamle agent"
        }
    }

## cheerio

为了方便对html元素的解析，使用cheerio来操作html元素。

    const cheerio = require('cheerio');
    const $ = chieerio.load('选择你需要爬取的类名');

简单来说，要使用cheerio来解析html，首先调用load方法将一个html片段传递给它，参数可以是一个完整的html文件内容，也可以是html元素片段。

所以，cheerio在项目中的位置就是，在request返回爬取内容之后，在回调函数中调用cheerio返回的html进行解析，提取其中的信息并进一步操作。

## http请求

现在我们需要构建一个http请求，在非登录状态下，一些网站通常都有功能限制，这个时候，我们可以在登录状态的控制台，把整个request header都复制下来。

### 批量爬取

    const fs = require('fs');
    const createHeader = require('./requestHeader');
    const request = require('request');

    function Request(Id) {
        const header = createHttpHeader(id);
        request(header, function (err, res) {
            Request(id++);

        })
    }

优化方式：

- 将连续的请求按一定的大小分块，例如一个请求块只发出十个请求。
- 使用定时器，在发出一个数量单位的请求之后，进程会暂停一段时间等待结果返回然后再发出请求，例如以5秒为单位，每次只爬取5个地址信息。

    var beign = Number(process.argv[2]);
    var end = Number(process.argv[3]);
    var tmp = begin;
    function RequestID() {
        var option = createHeader(begin);
        request(option, function (err, response) {
            if (err) {
                return;
            } 
            parseRes.processPage(response);
        });
        begin++;
        while(begin < end) {
            if (begin - tmp == 5) {
                setTimeout(RequestId, 5000);
                tmp = begin;
            } else {
                RequestID();
            }
        }
    }

上面的代码接收两个进程参数，为了后面的多进程架构做准备，begin和end活动单个进程爬取的范围。

### 多进程并行

为了提升效率，往往会考虑使用多个进程/线程进行并行爬取。我们可以使用child_process模块来实现一个简单的多进程模块。

    const child_process = require('child_process');
    const fs = require('fs');
    const tmp = 0;
    const process = [];
    const status = [];
    function start() {
        for (let i = 0; i < 5; i ++) {//使用child_process创建了5个线程
            process[i] = child_process.fork('worker.js', [tmp, tmp+50000]);
            tmp += 50000;
        }
    }
    start();
    
## 改进

我们可以使用async方法，将现有的异步操作改为promise的形式。

    let rp = function (header) {
        return new Promise(function (resolve, reject) {
            request(header, function (err, response) {
                if (err) {
                    reject(err);
                } else {
                    resolve(response);
                }
            })
        })
    }

之后将request方法封装成Promise的形式，将原来的request替换成rq方法。

    async function RequestId(id) {
        const optioin = createHeader(id);
        await rp(option).then(funciton (resopnse) {
            if (response.statusCode == 200) {
                paresRes.processPage(response);
            } else {

                console.log(begin)
            }
        }).catch(function (err) {
            console.log(err);
        })
    }
    async function RequestTest() {
        for (let i = 0; i < 100000; i++) {
            await RequestId(i);
        }
    }
    RequestTest();

这样就可实现顺序调用。